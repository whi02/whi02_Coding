
from google.colab import drive
drive.mount('/content/drive')

import kagglehub

# Download latest version
path = kagglehub.dataset_download("manjilkarki/deepfake-and-real-images")

print("Path to dataset files:", path)

import os

# 경로를 지정하여 해당 디렉토리의 파일 목록을 출력합니다.
dataset_path = "/root/.cache/kagglehub/datasets/manjilkarki/deepfake-and-real-images/versions/1"
print(os.listdir(dataset_path))

import tensorflow as tf
import numpy as numpy
import os

i = 256
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Dataset/Train",
    image_size = (i, i),
    batch_size = i,
    seed = 1234
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Dataset/Validation",
    image_size = (i, i),
    batch_size = i,
    seed = 1234
)
print(train_ds)

'''test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "C:/Users/user/Desktop/Dataset/test",
    image_size = (i, i),
    batch_size = i,
    subset = 'validation',
    validation_split = 0.2,
    seed = 1234
)'''

def before(i, answer):
  i = tf.cast(i/255.0, tf.float32)
  return i,answer

train_ds = train_ds.map(before)
val_ds = val_ds.map(before)
#test_ds = test_ds.map(before)

import os

# 데이터셋 디렉토리 확인
train_path = "/content/drive/MyDrive/deepfake_dataset/1/Dataset/Train"
validation_path = "/content/drive/MyDrive/deepfake_dataset/1/Dataset/Validation"

# 각 디렉토리의 파일 검사
for path in [train_path, validation_path]:
    for root, dirs, files in os.walk(path):
        for file in files:
            file_path = os.path.join(root, file)
            # 파일이 비어 있는지 확인
            if os.path.getsize(file_path) == 0:
                print(f"Empty file found: {file_path}")


import os
text = '''Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Fake/fake_4804.jpg
Empty file found': /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_62080.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_62081.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_62082.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_62083.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_62084.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_62085.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33787.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33788.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33789.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_3379.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33790.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33791.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33792.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33793.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33794.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33795.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33796.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33797.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33798.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_33799.jpg
Empty file found: /content/drive/MyDrive/deepfake_dataset/1/Dataset/Train/Real/real_14255.jpg'''
list = [text.replace('Empty file found: ', '').split('\n')]
for i in list:
  i = i[0]
  os.remove(i)

  AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)


'''import shutil

# 소스 및 대상 경로 정의
src = '/content/drive/MyDrive/deepfake_dataset/1/Dataset'
dst = '/content/drive/MyDrive/Dataset' # 대상을 MyDrive 내의 하위 폴더로 변경

# 디렉토리 이동
shutil.move(src, dst)'
'''

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D( 32, (3,3), padding = 'same', activation = 'relu',
input_shape = (256,256,3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D( 32, (3,3), padding = 'same', activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv2D( 32, (3,3), padding = 'same', activation = 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dense(128, activation = 'relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation ='sigmoid'),
])

model.compile(loss = "binary_crossentropy",
optimizer = 'adam', metrics = ['accuracy'])
model.fit(train_ds, validation_data = val_ds, epochs = 5, verbose = 1)

def 전처리함수(i, 정답):
  i = tf.cast(i/255.0, tf.float32)
  return i, 정답


train_ds = train_ds.map(전처리함수)
val_ds = val_ds.map(전처리함수)

import tensorflow as tf
print("GPU 사용 가능 여부:", tf.config.list_physical_devices('GPU'))

